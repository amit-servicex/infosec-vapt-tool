# VAPT Product Scaffold

VAPT Product Scaffold â€” Architecture & Guide
This repository is a modular VAPT engine you can run module-by-module today and chain into full pipelines later. Itâ€™s built to be plug-and-play, language-agnostic, and CI-friendly.
Folder Tree (at a glance)
export PYTHONPATH=/home/amitks/infosec-vapt-tool:$PYTHONPATH

vapt/
â”œâ”€ apps/                              # user-facing + orchestration services
â”‚  â”œâ”€ web-portal/                     # your admin UI (any framework)
â”‚  â”œâ”€ api/                            # REST API to accept jobs & expose results
â”‚  â”‚  â”œâ”€ main.py                      # FastAPI entry; POST /jobs, GET /runs/{id}
â”‚  â”‚  â”œâ”€ routes/                      # (future) route modules
â”‚  â”‚  â”œâ”€ services/                    # (future) business logic (DB, auth, s3â€¦)
â”‚  â”‚  â””â”€ schemas/                     # request/response Pydantic models
â”‚  â””â”€ worker/                         # job runner that executes pipelines
â”‚     â”œâ”€ worker.py                    # consumes queue; launches pipeline engine
â”‚     â””â”€ queue/                       # adapters (Redis/Rabbit/SQS)
â”‚
â”œâ”€ core/                              # engine & shared libraries
â”‚  â”œâ”€ pipeline/                       # pipeline engine (stages, retries)
â”‚  â”‚  â”œâ”€ engine.py                    # runs pipeline -> executes modules
â”‚  â”‚  â”œâ”€ registry.py                  # discovers plugin manifests
â”‚  â”‚  â””â”€ contracts.py                 # Pydantic models (RunConfig, Finding, Artifact)
â”‚  â”œâ”€ plugins/                        # PLUGGABLE MODULES (scan/attack/report/etc.)
â”‚  â”‚  â”œâ”€ web/                         # web security modules
â”‚  â”‚  â”‚  â”œâ”€ zap/
â”‚  â”‚  â”‚  â”‚  â”œâ”€ main.py                # module trigger (stdin JSON -> stdout JSON)
â”‚  â”‚  â”‚  â”‚  â”œâ”€ subtasks/              # internal steps (spider, ajax, active)
â”‚  â”‚  â”‚  â”‚  â””â”€ manifest.yaml          # id, inputs, outputs, capabilities
â”‚  â”‚  â”‚  â”œâ”€ nuclei/
â”‚  â”‚  â”‚  â”‚  â”œâ”€ main.py
â”‚  â”‚  â”‚  â”‚  â”œâ”€ subtasks/
â”‚  â”‚  â”‚  â”‚  â””â”€ manifest.yaml
â”‚  â”‚  â”‚  â”œâ”€ sqlmap/
â”‚  â”‚  â”‚  â”œâ”€ ffuf/
â”‚  â”‚  â”‚  â””â”€ content-discovery/        # feroxbuster/dirsearch
â”‚  â”‚  â”œâ”€ mobile/
â”‚  â”‚  â”‚  â”œâ”€ ios/mobsf/
â”‚  â”‚  â”‚  â””â”€ android/mobsf/
â”‚  â”‚  â”œâ”€ api/                         # OpenAPI/GraphQL/Schemathesis (future)
â”‚  â”‚  â””â”€ oast/interactsh/             # out-of-band callbacks (future)
â”‚  â”œâ”€ reporting/
â”‚  â”‚  â”œâ”€ aggregate.py                 # merge tool outputs -> unified findings[]
â”‚  â”‚  â”œâ”€ writers/
â”‚  â”‚  â”‚  â”œâ”€ html_writer.py
â”‚  â”‚  â”‚  â”œâ”€ sarif_writer.py
â”‚  â”‚  â”‚  â””â”€ json_writer.py
â”‚  â”‚  â””â”€ templates/                   # HTML templates, logos, compliance annex
â”‚  â”œâ”€ mapping/                        # CWE/OWASP/PCI/ISO/GDPR crosswalks
â”‚  â”‚  â”œâ”€ cwe.json
â”‚  â”‚  â”œâ”€ owasp_top10.json
â”‚  â”‚  â””â”€ compliance_map.yaml
â”‚  â”œâ”€ utils/                          # shared helpers (proc, io, redact, auth)
â”‚  â””â”€ validators/                     # schema validators for modules & pipelines
â”‚
â”œâ”€ pipelines/                         # declarative pipelines (plug-and-play)
â”‚  â”œâ”€ web_default.yaml
â”‚  â”œâ”€ web_active_plus_oast.yaml
â”‚  â”œâ”€ api_fuzz.yaml
â”‚  â””â”€ mobile_basic.yaml
â”‚
â”œâ”€ configs/                           # runtime configs (env-agnostic)
â”‚  â”œâ”€ policies/                       # scan policies (e.g., zap active policies)
â”‚  â”œâ”€ wordlists/                      # optional local lists (can symlink SecLists)
â”‚  â”œâ”€ nuclei/                         # nuclei config overrides
â”‚  â””â”€ auth/                           # login scripts, storageState, csrf names
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ runs/
â”‚  â”‚  â””â”€ {run_id}/                    # one folder per run (immutable)
â”‚  â”‚     â”œâ”€ inputs/                   # scope, pipeline snapshot, redacted creds
â”‚  â”‚     â”œâ”€ workspace/                # tool working dirs
â”‚  â”‚     â”œâ”€ artifacts/                # raw tool outputs (JSON/HTML/TXT)
â”‚  â”‚     â”œâ”€ findings.json             # normalized findings
â”‚  â”‚     â”œâ”€ manifest.json             # tool versions, flags, start/end, hashes
â”‚  â”‚     â””â”€ reports/                  # html, sarif, csv/xlsx
â”‚  â””â”€ cache/                          # nuclei templates, fingerprints, tmp
â”‚
â”œâ”€ infra/                             # deployment bits
â”‚  â”œâ”€ docker/
â”‚  â”œâ”€ k8s/
â”‚  â””â”€ ci/                             # CI pipelines (lint, unit, e2e, release)
â”‚
â”œâ”€ scripts/                           # dev scripts (bootstrap, doctor, local-run)
â”œâ”€ docs/                              # architecture, module authoring guide, API docs
â””â”€ pyproject.toml / requirements.txt
Why this architecture?
Separation of concerns
apps/ hosts the interfaces (API, worker, web portal).
core/ holds the engine logic and plugins (scanners, attacks, reporting).
pipelines/ describes what to run; core/ implements how to run.
Pluggable modules
Each tool lives in core/plugins/**/ with a manifest.yaml and a single trigger main.py.
Modules are language-agnostic: they read JSON on stdin and return JSON on stdout.
Swap or add tools (e.g., ZAP â†”ï¸ Burp Enterprise API, ffuf â†”ï¸ wfuzz) without touching the engine.
Reproducibility
Every run writes an immutable data/runs/{run_id}/manifest.json (tool versions, params, timestamps).
Artifacts and findings.json are preserved for audit, retest, and reporting.
Scales from CLI to product
Start by running modules individually; later, the worker and pipeline engine orchestrate end-to-end flows.
infra/ gives you a clean path to Docker/Kubernetes/CI.
Key Components
apps/api
Small FastAPI stub to submit jobs (pipelines + inputs) and query run state.
Replace in-memory store with DB/queue when you wire real workers.
apps/worker
Long-running process that consumes a queue and calls the pipeline engine.
Adapter folder for Redis/Rabbit/SQS so you can switch infra without code churn.
core/pipeline
contracts.py: Pydantic models for RunConfig, Finding, Artifact, etc.
registry.py: discovers plugins via their manifest.yaml.
engine.py: executes a list of modules (sequential placeholder; later: DAG, retries, budgets, parallelism).
core/plugins
Where the actual scanners live (web, mobile, api, oast).
Each module:
manifest.yaml â†’ identity, inputs, outputs, capabilities.
main.py â†’ the main trigger (reads a ModuleInput, emits a ModuleOutput).
subtasks/ â†’ optional breakdown (e.g., ZAP spider, Ajax spider, active scan).
core/reporting
aggregate.py merges outputs into a unified findings[] list.
Writers: html_writer.py, sarif_writer.py, json_writer.py.
templates/ for branding, compliance annex, and print-friendly CSS.
mapping/
Static crosswalks (CWE, OWASP Top 10, PCI/ISO/GDPR) to enrich each finding with industry tags.
pipelines/
YAML recipes describing stages and which plugins to run.
Keep product presets here (e.g., web_default, web_active_plus_oast, api_fuzz).
configs/
Policy files, wordlists, Nuclei overrides, and auth recipes (login scripts, storageState, CSRF names).
data/
Single source of truth for run outputs. Everything needed to reproduce a run is here.
JSON I/O Contract (modules)
Input (stdin)
{
  "run_id": "2025-08-21T12-00Z-abc",
  "workdir": "data/runs/2025-08-21T12-00Z-abc/workspace",
  "inputs": { "target_url": "https://example.com" },
  "env": { }
}
Output (stdout)
{
  "status": "ok",
  "artifacts": [
    { "path": "data/runs/.../workspace/zap-baseline.json",
      "description": "ZAP baseline output",
      "content_type": "application/json" }
  ],
  "findings": [
    { "id": "XSS-REFLECTED",
      "title": "Reflected XSS",
      "severity": "high",
      "location": "https://example.com/search?q=",
      "tool": "zap",
      "rule_id": "12345",
      "cwe": "CWE-79",
      "owasp": "A03:2021" }
  ],
  "stats": { "duration_sec": 120 }
}
Keep modules pure: no global state, write artifacts only inside workdir, return results in JSON.
Quick Start
Create & activate env
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
Run API stub (optional)
uvicorn apps.api.main:app --reload
# POST /jobs with {"pipeline_id":"web_default","inputs":{"target_url":"https://example.com"}}
Run a module directly (individual testing)
python core/plugins/web/nuclei/main.py --stdin <<'JSON'
{"run_id":"dev1","workdir":"data/runs/dev1/workspace","inputs":{},"env":{}}
JSON
Generate a simple HTML report from findings.json
# python - <<'PY'
from core.reporting.writers.html_writer import write
import json, pathlib
p = pathlib.Path("data/runs/dev1/workspace/findings.json")
data = json.loads(p.read_text()) if p.exists() else {"findings":[]}
write(data, "data/runs/dev1/reports/report.html")
# PY
Design Principles
Plug & Play: Add modules without touching the engine.
Stateless Workers: Everything required for a run is in RunConfig + configs/.
Deterministic: Same inputs â†’ same outputs. Pin template/tool versions in manifest.json.
Safety First: Enforce allow/deny lists; require scope acknowledgement; add rate limits.
Auditability: Keep raw artifacts and a human-readable report; export SARIF for CI.
Adding a New Plugin (example)
Create core/plugins/<domain>/<tool>/.
Add manifest.yaml:
id: web.mytool
version: "0.1.0"
type: scan
inputs: [ target_url ]
outputs:
  - artifacts: ["mytool.json"]
  - findings: true
capabilities: [ passive ]
Implement main.py:
Read JSON from stdin
Run the tool (or stub)
Save artifacts under workdir
Print a valid ModuleOutput JSON to stdout
Reference it in a pipeline under pipelines/*.yaml.


docker build -t whz/vapt-zap:0.1.0     core/plugins/web/zap
docker build -t whz/vapt-nuclei:0.1.0  core/plugins/web/nuclei
docker build -t whz/vapt-report-html:0.1.0 core/reporting/report_module


Week 1 Progress Overview:

Core Engine & Pipeline Setup:

Core Engine Configuration: Set up the core pipeline engine using RunConfig, ModuleInput/Output, Finding, Artifact, and ModuleResult in core/pipeline/contracts.py. Configured a sequential pipeline engine in core/pipeline/engine.py for processing jobs.

Plugin Registration & Discovery:

Dynamic Module Discovery: Implemented dynamic module discovery in core/pipeline/registry.py using manifest.yaml. This allows automatic registration and discovery of plugins located in core/plugins/**/manifest.yaml.

Job Queueing Setup:

In-memory Queue: Set up a basic job queueing system for worker execution, using an in-memory store for the initial version. A more robust solution (like Redis) is planned for future updates.

ZAP Baseline Integration:

ZAP Baseline Scan: Integrated the ZAP baseline scan for passive scanning. The wrapper was created, and the ZAP tool can be triggered via the CLI. After installing ZAP (docker or local), it generates real findings.

Nuclei Integration:

Nuclei Template Scanning: Integrated Nuclei for automated vulnerability scanning with templates. The Nuclei module was created, and it correctly parses the jsonl files, running against multiple templates.

Basic Reporting:

HTML Reporting: Implemented an HTML report generation system. The report is generated from findings, but the initial issue was that the findings were not being correctly captured in the HTML output.

Current Issue:

Currently, the Nuclei scan results are being generated but not properly reflected in the HTML report. Hereâ€™s a breakdown of the problem:

Nuclei Scan Output: The Nuclei module (web.nuclei.basic) runs successfully, and the findings are stored in a nuclei.jsonl file. However, the HTML report generation module (report.html) is not capturing these findings correctly and not displaying them in the generated HTML report.

Debugging Logs:

Nuclei runs without errors but reports no findings in its output.

The findings in the JSON file are empty, likely because Nuclei doesnâ€™t find any vulnerabilities based on the current templates and target URL.

Potential Reasons:

Nuclei Templates: The templates used for scanning may not be sufficient or aligned with the target, resulting in no findings. The templates may need to be expanded, or more targeted templates should be used for the specific URL.

Pipeline Configuration: There could be a configuration issue in the pipeline setup where the Nuclei findings are not properly fed into the report generation system. The report.html module might not be processing the nuclei.jsonl file correctly.

Permissions/Write Path: There may be permission issues, particularly related to the Docker container's ability to write to the artifacts directory, preventing findings from being captured or displayed.

Proposed Solution:

Check Nuclei Findings:

Verify that the nuclei.jsonl file is being populated with valid findings and that Nuclei is properly scanning the URL. You can manually test the scanning by running the Nuclei tool outside the pipeline with specific templates and checking the output.

Validate Report Generation:

Ensure the main.py script is correctly handling the findings from nuclei.jsonl and generating the HTML report. If necessary, add debug logs to confirm the content of nuclei.jsonl and see how itâ€™s processed for the HTML output.

Review Template Selection:

Check the Nuclei template set being used and ensure it covers the vulnerabilities relevant to the target site. You may need to modify the pipeline configuration to include high-signal templates, such as those targeting misconfigurations, common vulnerabilities, or outdated software.

Ensure Correct Permissions:

Confirm that Docker has write permissions for the artifacts directory (/artifacts), and that Nucleiâ€™s output files are correctly placed in this directory.

Once these steps are followed, you should be able to see the findings in the HTML report generated at /artifacts/report.html. If the issue persists, more detailed logs or template adjustments might be required. Let me know if you need further guidance!




Week-2 Â· Day-8 â€” ZAP Full Scan + Debugging (Testfire)

Date: 2025-08-26
Scope: Enable ZAP active scanning, add rich debug logs, and verify runs against https://testfire.net.
Outputs: HTML/JSON reports + normalized findings + streaming debug log.

What changed (Changelog)

core/plugins/web/zap/main.py

Added mode: "full" support (kept baseline intact).

Docker fallback now defaults to ghcr.io/zaproxy/zaproxy:stable.

New debug log at artifacts/zap-debug.log (heartbeat + full command).

Hard timeout guard (derived from max_duration_min).

Normalized findings saved to workspace/zap/findings.json.

New inputs:

ajax_spider: bool

max_duration_min: int

risk_threshold: "informational"|"low"|"medium"|"high"

include_patterns: string[]

exclude_patterns: string[]

debug: bool (adds -d to wrapper)

extra_zap: string[] (optional, extra -z configs to ZAP)

Environment switches

ZAP_DOCKER_IMAGE (default: ghcr.io/zaproxy/zaproxy:stable)

ZAP_DOCKER_EXTRA_ARGS (e.g., --network host, or host gateway mapping)

Directory layout (per run)



 Use official ZAP image on GHCR
export ZAP_DOCKER_IMAGE="ghcr.io/zaproxy/zaproxy:stable"

# If scanning services on your Linux localhost (lets ZAP reach 127.0.0.1)
# (Comment this out when not needed.)
export ZAP_DOCKER_EXTRA_ARGS="--network host"

# Optional: surface verbose wrapper logs in our module
export DEBUG_ZAP=1
export SQLMAP_DOCKER_IMAGE="sqlmapproject/sqlmap"


# Try Parrotâ€™s image
docker pull parrotsec/sqlmap
docker run --rm parrotsec/sqlmap sqlmap --version

# Tell your plugin to use it
export SQLMAP_DOCKER_IMAGE="parrotsec/sqlmap"
export FFUF_DOCKER_IMAGE=secsi/ffuf:2.0.0



ğŸ”’ VAPT Tool Progress

Integrated Scanners

ZAP (passive/active web scan)

Nuclei (template-based vuln checks)

sqlmap (SQLi exploitation)

ffuf (fuzzing & content discovery)
â†’ Each runs in Docker and saves results in workspace/<tool>/findings.json.

Aggregator Module (core/plugins/report/aggregate/main.py)

Reads outputs from all four tools.

Normalizes URLs (scheme/host/path/query).

Dedupes by (host, path, parameter?, type).

Canonical finding schema fields:
id, type, url, norm{}, method, parameter, location,
severity, confidence, sources[], evidence, tags[],
timestamp, compliance{}
Merges overlapping findings, escalates severity/confidence, trims evidence.

Writes merged file â†’ workspace/findings.json.

Emits artifacts/aggregate-summary.json and artifacts/aggregate-debug.log.

âœ… Last run: 37 raw â†’ 5 unique findings (32 deduped).

Compliance Expert Agent (core/agents/compliance_expert/main.py)

Reads workspace/findings.json.

Loads compliance packs from configs/crosswalks/ (pci.json, iso27001.json, gdpr.json).

Maps findings to compliance clauses (PCI DSS, ISO 27001, GDPR).

Adds rationale + evidence refs per finding.

Writes enriched file â†’ workspace/findings.enriched.json.

Emits artifacts/compliance-expert-summary.json + compliance-expert-debug.log.

Currently warns â€œno crosswalk packs loadedâ€ when the JSONs are missing.

Compliance Crosswalks

configs/crosswalks/pci.json â†’ e.g., sqli.* â†’ PCI 6.5.1/11.3.1

configs/crosswalks/iso27001.json â†’ e.g., xss.* â†’ ISO A.8.25

configs/crosswalks/gdpr.json â†’ DSL rules (regex evidence â†’ GDPR Art.32, etc.)

source ./setup_vapt.sh

cd /home/amitks/infosec-vapt-tool/core/plugins/web/nuclei/
docker build -f Dockerfile.nuclei-wrapper -t vapt-nuclei:wrapper .
